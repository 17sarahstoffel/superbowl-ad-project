{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96cf355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47093d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resources\n",
    "main_df = pd.read_csv(\"Resources/Cleaned Data.csv\")\n",
    "ad_tag_view_counts_df = pd.read_csv(\"Resources/ad_tag_view_counts.csv\")\n",
    "ad_tag_counts_df = pd.read_csv(\"Resources/ad_tag_counts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc45b6",
   "metadata": {},
   "source": [
    "# Bar Charts\n",
    "\n",
    "#### The most common ads to be made in 2000-2020 was funny ads and ads that showed the product quickly. The same can be said for the ads that were viewed the most. This makes sense because if there are more types of one ad made, then it would be logical to believe that it would have more views. It was a surprise for us to see that ads that showed the product quickly had the most views by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b683f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one bar chart to get the total number of ads by type\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.bar(ad_tag_counts_df.keys(), ad_tag_counts_df.iloc[0, :] , color=\"blue\", align=\"center\", width=0.5)\n",
    "plt.xticks([value for value in ad_tag_counts_df.keys()], rotation =\"vertical\", fontsize=12)\n",
    "plt.title(\"Total Number of Ad by Type\", fontsize=18)\n",
    "plt.xlabel(\"Type of Ad\", fontsize=12)\n",
    "plt.ylabel(\"Total Number of Ads\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one bar chart to get the total number of views by ads type\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.bar(ad_tag_view_counts_df.keys(), ad_tag_view_counts_df.iloc[0, :] , color=\"blue\", align=\"center\", width=0.5)\n",
    "plt.xticks([value for value in ad_tag_view_counts_df.keys()], rotation =\"vertical\", fontsize=12)\n",
    "plt.title(\"Total Number of Views by Ad Type\", fontsize=18)\n",
    "plt.xlabel(\"Type of Ad\", fontsize=12)\n",
    "plt.ylabel(\"Total Number of Views by Ad Type\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb39bad",
   "metadata": {},
   "source": [
    "# Pie Charts\n",
    "\n",
    "#### From the pie charts we can see that almost 70% of the ads had the tag of funny. The funny ads also had over 70% of the total views. This would make sense because if one type was made more often it would make sense if it also had the most views.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of funny vs not funny by type\n",
    "\n",
    "# funny\n",
    "funny_ads=len(main_df.loc[main_df[\"Funny\"] == True])\n",
    "\n",
    "# not funny\n",
    "not_funny_ads=len(main_df.loc[main_df[\"Funny\"] == False])\n",
    "\n",
    "# Pie chart showing funny vs not funny by type\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Labels for each slice of the pie chart\n",
    "labels = [\"Funny\", \"Not Funny\"]\n",
    "\n",
    "# Values representing the number of funny and not funny ad videos\n",
    "sizes = [funny_ads, not_funny_ads]\n",
    "\n",
    "# colors for each slice of the pie chart\n",
    "colors = [\"yellow\", \"lightskyblue\"]\n",
    "\n",
    "# funny separated from not funny\n",
    "explode = [0.1, 0]\n",
    "\n",
    "# create pie chart from values\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "         autopct=\"%1.1f%%\", shadow=True, startangle=90)\n",
    "\n",
    "plt.title(\"Number of Ads Tagged As Funny\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of views by funny and not funny ads\n",
    "funny_views=main_df.loc[main_df[\"Funny\"] == True] [\"View Counts\"].sum()\n",
    "not_funny_views=main_df.loc[main_df[\"Funny\"] == False] [\"View Counts\"].sum()\n",
    "\n",
    "# pie chart of funny vs not funny by viewcount\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "# Labels for each slice of the pie chart\n",
    "labels = [\"Funny\", \"Not Funny\"]\n",
    "\n",
    "# Values representing the viewcounts of funny and not funny ad videos\n",
    "sizes = [funny_views, not_funny_views]\n",
    "\n",
    "# colors for each slice of the pie chart\n",
    "colors = [\"green\", \"red\"]\n",
    "\n",
    "# funny separated from not funny\n",
    "explode = [0.1, 0]\n",
    "\n",
    "# create pie chart from values\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "         autopct=\"%1.1f%%\", shadow=True, startangle=90)\n",
    "\n",
    "plt.title(\"Funny and Not Funny Ads by View Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45548eb5",
   "metadata": {},
   "source": [
    "# Hypothesis Test\n",
    "\n",
    "###### Hypothesis\n",
    "Ads that contain the tag 'Funny' would receive more views on YouTube.\n",
    "\n",
    "###### Null Hypothesis\n",
    "There is no statistically significant difference in view counts between ads that contain the 'Funny' tag and those that do not contain the 'Funny' tag.\n",
    "\n",
    "###### Results\n",
    "Because the calculated P-Value is exactly zero, we reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e86b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the relevant data from the main DataFrame\n",
    "funnySeries = main_df.loc[main_df[\"Funny\"] == True, :][\"View Counts\"]\n",
    "unfunnySeries = main_df.loc[main_df[\"Funny\"] == False, :][\"View Counts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab0811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the hypothesis test series\n",
    "obsSeries = pd.Series(data = [sum(funnySeries), sum(unfunnySeries)], name = \"observed\")\n",
    "expSeries = pd.Series(data = [sum(obsSeries) / 2, sum(obsSeries) / 2], name = \"expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3829a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a Chi-Squared test because this is comparing categorical data\n",
    "pValue = st.chisquare(obsSeries, expSeries)[1]\n",
    "\n",
    "# display the p-value\n",
    "print(f\"P-Value = {pValue:,.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f0ec44",
   "metadata": {},
   "source": [
    "### Batch Hypothesis Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct list of ad categories\n",
    "categories = [\"Funny\", \"Shows Product Quickly\", \"Celebrity\", \"Danger\", \"Animals\", \"Use Sex\"]\n",
    "\n",
    "# declare the p-value lists\n",
    "chi_square_pvalues = []\n",
    "student_ttest_pvalues = []\n",
    "mannwhitneyu_pvalues = []\n",
    "kruskal_pvalues = []\n",
    "\n",
    "# declare the summary statistics lists\n",
    "maxTrue_list = []\n",
    "minTrue_list = []\n",
    "sumTrue_list = []\n",
    "meanTrue_list = []\n",
    "medianTrue_list = []\n",
    "maxFalse_list = []\n",
    "minFalse_list = []\n",
    "sumFalse_list = []\n",
    "meanFalse_list = []\n",
    "medianFalse_list = []\n",
    "\n",
    "# iterate through the ad categories\n",
    "for category in categories:\n",
    "    \n",
    "    # extract the series\n",
    "    whenTrue = main_df.loc[main_df[category] == True, :][\"View Counts\"]\n",
    "    whenFalse = main_df.loc[main_df[category] == False, :][\"View Counts\"]\n",
    "    \n",
    "    # construct the Chi Square observed series\n",
    "    observed = pd.Series(data = [sum(whenTrue), sum(whenFalse)], name = \"observed\")\n",
    "    expected = pd.Series(data = [sum(observed) / 2, sum(observed) / 2], name = \"expected\")\n",
    "    \n",
    "    # calculate the P-Value with a Chi Square Test\n",
    "    chi_square_pvalues.append(st.chisquare(observed, expected)[1])\n",
    "    \n",
    "    # calculate the P-Value with a Student T-Test\n",
    "    student_ttest_pvalues.append(st.ttest_ind(whenFalse, whenTrue)[1])\n",
    "    \n",
    "    # calculate the P-Value with a Mann-Whitney U Test\n",
    "    mannwhitneyu_pvalues.append(st.mannwhitneyu(whenTrue, whenFalse, alternative = \"less\")[1])\n",
    "    \n",
    "    # calculate the P-Value with a Kruskal-Wallis Test\n",
    "    kruskal_pvalues.append(st.kruskal(whenTrue, whenFalse)[1])\n",
    "    \n",
    "    # calculate the summary statistics\n",
    "    maxTrue_list.append(max(whenTrue))\n",
    "    minTrue_list.append(min(whenTrue))\n",
    "    sumTrue_list.append(sum(whenTrue))\n",
    "    meanTrue_list.append(whenTrue.mean())\n",
    "    medianTrue_list.append(whenTrue.median())\n",
    "    maxFalse_list.append(max(whenFalse))\n",
    "    minFalse_list.append(min(whenFalse))\n",
    "    sumFalse_list.append(sum(whenFalse))\n",
    "    meanFalse_list.append(whenFalse.mean())\n",
    "    medianFalse_list.append(whenFalse.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c9ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame from the results then transpose it\n",
    "pvalues_df = pd.DataFrame({\n",
    "                    \"Tag\": categories,\n",
    "                    \"Chi Square\": [f\"{member:,.4f}\" for member in chi_square_pvalues],\n",
    "                    \"Student T-Test\": [f\"{member:,.4f}\" for member in student_ttest_pvalues],\n",
    "                    \"Mann-Whitney U\": [f\"{member:,.4f}\" for member in mannwhitneyu_pvalues],\n",
    "                    \"Kruskal-Wallis\": [f\"{member:,.4f}\" for member in kruskal_pvalues],\n",
    "                    \"Max (True)\": [f\"{member:,.0f}\" for member in maxTrue_list],\n",
    "                    \"Max (False)\": [f\"{member:,.0f}\" for member in maxFalse_list],\n",
    "                    \"Min (True)\": [f\"{member:,.0f}\" for member in minTrue_list],\n",
    "                    \"Min (False)\": [f\"{member:,.0f}\" for member in minFalse_list],\n",
    "                    \"Sum (True)\": [f\"{member:,.0f}\" for member in sumTrue_list],\n",
    "                    \"Sum (False)\": [f\"{member:,.0f}\" for member in sumFalse_list],\n",
    "                    \"Mean (True)\": [f\"{member:,.0f}\" for member in meanTrue_list],\n",
    "                    \"Mean (False)\": [f\"{member:,.0f}\" for member in meanFalse_list],\n",
    "                    \"Median (True)\": [f\"{member:,.0f}\" for member in medianTrue_list],\n",
    "                    \"Median (False)\": [f\"{member:,.0f}\" for member in medianFalse_list]}).T\n",
    "\n",
    "# set the column headers to the tags\n",
    "pvalues_df.columns = pvalues_df.iloc[0, :]\n",
    "\n",
    "# remove the tags row\n",
    "pvalues_df = pvalues_df.iloc[1:, :]\n",
    "\n",
    "# display the DataFrame\n",
    "print(\"P-Values & Statistics for View Counts\")\n",
    "pvalues_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8b0bf",
   "metadata": {},
   "source": [
    "### Adjustment to Hypothesis Testing\n",
    "After conversations with Dominic LaBella and Colin Barquist we found that our original hypothesis test and subsequent batch tests were established on incorrect assumptions. The results from those tests should not be taken at face value. The sheer difference in volume of videos tagged as 'x' vs 'y' caused such a vast difference in the View Counts that a realistic analysis was improbable. To fix this we take a random sampling from each subset so we're comparing series of equal sizes then run a Chi Square test on the resulting series. This is repeated multiple times to account for the random sampling.\n",
    "\n",
    "Because the results consistently show P-Values above the 0.05 threshold we fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the View Counts quartiles and IQR\n",
    "quartiles = main_df[\"View Counts\"].quantile([0.25, 0.50, 0.75])\n",
    "lower = quartiles[0.25]\n",
    "upper = quartiles[0.75]\n",
    "IQR = upper - lower\n",
    "\n",
    "# calculate the boundaries\n",
    "lBound = lower - (1.5 * IQR)\n",
    "uBound = upper + (1.5 * IQR)\n",
    "\n",
    "# remove potential outliers from the View Counts\n",
    "vcFiltered_df = main_df.loc[(main_df[\"View Counts\"] >= lBound) & (main_df[\"View Counts\"] <= uBound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the analysis iteration count\n",
    "analysisCount = 10\n",
    "\n",
    "# set the minimum allowed iterations\n",
    "minAllowed = 20\n",
    "\n",
    "# set the iteration's fraction of source\n",
    "itrFraction = 0.30\n",
    "\n",
    "# calculate the row count of the resulting DataFrame\n",
    "rowCount = analysisCount + 5\n",
    "\n",
    "# create the resulting DataFrame\n",
    "stats_df = pd.DataFrame({\"Iteration\": [value for value in range(rowCount)]})\n",
    "\n",
    "# change the last 'Iteration' entry\n",
    "stats_df.iloc[len(stats_df) - 5, 0] = \"Max\"\n",
    "stats_df.iloc[len(stats_df) - 4, 0] = \"Min\"\n",
    "stats_df.iloc[len(stats_df) - 3, 0] = \"Mean\"\n",
    "stats_df.iloc[len(stats_df) - 2, 0] = \"Median\"\n",
    "stats_df.iloc[len(stats_df) - 1, 0] = \"StDev\"\n",
    "\n",
    "# iterate through the analysis x times\n",
    "for category in categories:\n",
    "    \n",
    "    # filter the DataFrame to True/False\n",
    "    whenTrue_df = vcFiltered_df.loc[vcFiltered_df[category] == True, :]\n",
    "    whenFalse_df = vcFiltered_df.loc[vcFiltered_df[category] == False, :]\n",
    "    \n",
    "    # calculate the number of iterations\n",
    "    itrCount = 0\n",
    "    if len(whenTrue_df) <= len(whenFalse_df):\n",
    "        itrCount = len(whenTrue_df) - int(itrFraction * len(whenTrue_df))\n",
    "    else:\n",
    "        itrCount = len(whenFalse_df) - int(itrFraction * len(whenFalse_df))\n",
    "    \n",
    "    # initialize the data list\n",
    "    column = []\n",
    "    \n",
    "    # ensure the iteration count isn't too small\n",
    "    if itrCount >= minAllowed:\n",
    "    \n",
    "        # iterate through the defined categories\n",
    "        for i in range(analysisCount):\n",
    "            \n",
    "            # initialize the bins\n",
    "            tBin = 0\n",
    "            fBin = 0\n",
    "            \n",
    "            # assemble the bins\n",
    "            for j in range(itrCount):\n",
    "                \n",
    "                # extract the associated view counts\n",
    "                tValue = whenTrue_df.sample()[\"View Counts\"].tolist()[0]\n",
    "                fValue = whenFalse_df.sample()[\"View Counts\"].tolist()[0]\n",
    "\n",
    "                # increment the corresponding bin\n",
    "                if tValue > fValue:\n",
    "                    tBin += 1\n",
    "                elif tValue < fValue:\n",
    "                    fBin += 1\n",
    "            \n",
    "            # calculate the expected bin\n",
    "            eBin = (tBin + fBin) / 2\n",
    "            \n",
    "            # construct the Chi Square series\n",
    "            observed = pd.Series(data = [tBin, fBin], name = \"Observed\")\n",
    "            expected = pd.Series(data = [eBin, eBin], name = \"Expected\")\n",
    "            \n",
    "            # calculate the Chi Square p-value\n",
    "            column.append(st.chisquare(observed, expected)[1])\n",
    "        \n",
    "        # calculate stats\n",
    "        maxValue = max(column)\n",
    "        minValue = min(column)\n",
    "        meanValue = np.mean(column)\n",
    "        medianValue = np.median(column)\n",
    "        stDevValue = np.std(column)\n",
    "        \n",
    "        # add a row for p-value stats\n",
    "        column.append(maxValue)\n",
    "        column.append(minValue)\n",
    "        column.append(meanValue)\n",
    "        column.append(medianValue)\n",
    "        column.append(stDevValue)\n",
    "        \n",
    "        # add the data list to the DataFrame\n",
    "        stats_df[category] = [f\"{value:,.3f}\" for value in column]\n",
    "\n",
    "stats_df.style.hide(axis = \"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b6bcc0",
   "metadata": {},
   "source": [
    "# Regression Line to See the Trends of Length of SuperBowl Ads\n",
    "\n",
    "#### We created four different regression lines to see some of the different trends with the length of SuperBowl Ads from 2000-2020. The first scatter plot looks at all the ads and graphs them by year. While their is a good portion of the ads are around the same length, there are some outliers that affected the linear regression. We found the outliers and then created a second scatter plot to see how much of an affect the outliers had. Our slope went from 1.82 to 1.16, which did flatten the line but it still had a signficant slope. We noticed that a lot of the outliers tended to be in the later years of the data so we split the data into two pieces, 2000-2010 ads and 2011-2020 ads. From there could see that the 2000-2010 regression line was pretty flat (0.31) compared to the 2011-2020 regression (2.58). This lead us to think that the longer adds have become more common in since 2011. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating regression values\n",
    "(slope, intercept, rvalue, pvalue, stderr) = st.linregress(main_df[\"Year\"], main_df[\"Duration (seconds)\"])\n",
    "\n",
    "regress_value = main_df[\"Year\"] * slope + intercept\n",
    "\n",
    "#creating the linear regression equation\n",
    "line_eqn = \"y = \" + str(round(slope,2)) + \"x + \" + str(round(intercept,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e03fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot with regression line\n",
    "plt.scatter(main_df[\"Year\"], main_df[\"Duration (seconds)\"],)\n",
    "\n",
    "plt.plot(main_df[\"Year\"], regress_value, \"r-\")\n",
    "plt.annotate(line_eqn, (2000,130), fontsize= 15,  color = \"red\")\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Length of Ad (seconds)\")\n",
    "plt.title(\"Length of SuperBowl Ads Over the Years\")\n",
    "plt.ylim(20,220)\n",
    "\n",
    "plt.xticks(np.arange(2000, 2021, step=1), rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052bf514",
   "metadata": {},
   "source": [
    "### Checking for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc25721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the quartiles and IQR\n",
    "quartiles = main_df[\"Duration (seconds)\"].quantile([0.25, 0.5, 0.75])\n",
    "lower_quar = quartiles[.25]\n",
    "upper_quar = quartiles[0.75]\n",
    "iqr = upper_quar - lower_quar\n",
    "\n",
    "#Creating the bounds\n",
    "lower_bound = lower_quar - (1.5 * iqr)\n",
    "upper_bound = upper_quar + (1.5 * iqr)\n",
    "\n",
    "print(f'The lower quartile of duration of ads is: {lower_quar}.')\n",
    "print(f'The upper quartile of duration of ads is: {upper_quar}.')\n",
    "print(f'Values below {round(lower_bound,2)} could be outliers.')\n",
    "print(f'Values above {round(upper_bound,2)} could be outliers.')\n",
    "\n",
    "#creating a dataframe with the outliers\n",
    "outlier_lengths = main_df.loc[(main_df['Duration (seconds)'] < lower_bound) | (main_df['Duration (seconds)'] > upper_bound)]\n",
    "outlier_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a43835",
   "metadata": {},
   "source": [
    "### Regression Line without Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a3d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe without outliers\n",
    "no_outliers = main_df.drop(labels = [0,14,27,79,100,189,197], axis=0)\n",
    "\n",
    "#creating regression values\n",
    "(outlier_slope, outlier_intercept, outlier_rvalue, outlier_value, outlier_stderr) = st.linregress(\n",
    "    no_outliers[\"Year\"], no_outliers[\"Duration (seconds)\"])\n",
    "\n",
    "regress_value = no_outliers[\"Year\"] * outlier_slope + outlier_intercept\n",
    "\n",
    "#creating the linear regression equation\n",
    "line_eqn = \"y = \" + str(round(outlier_slope,2)) + \"x + \" + str(round(outlier_intercept,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot with regression line\n",
    "plt.scatter(no_outliers[\"Year\"], no_outliers[\"Duration (seconds)\"],)\n",
    "\n",
    "plt.plot(no_outliers[\"Year\"], regress_value, \"r-\")\n",
    "plt.annotate(line_eqn, (2000,125), fontsize= 15,  color = \"red\")\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Length of Ad (seconds)\")\n",
    "plt.title(\"Length of SuperBowl Ads Over the Years\")\n",
    "plt.ylim(20,220)\n",
    "\n",
    "plt.xticks(np.arange(2000, 2021, step=1), rotation = 90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f9b4ff",
   "metadata": {},
   "source": [
    "## Looking at the Data 10 Years at a Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting the data in order of year\n",
    "by_years_df= main_df.sort_values(by=[\"Year\"])\n",
    "\n",
    "\n",
    "#creating bins\n",
    "bins = [2000,2010,2020]\n",
    "\n",
    "group_names = [\"2000-2010\", \"2011-2020\"]\n",
    "\n",
    "by_years_df[\"Year Grouping\"]= pd.cut(by_years_df[\"Year\"], bins, labels = group_names, include_lowest= True)\n",
    "by_years_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5cf5a8",
   "metadata": {},
   "source": [
    "### 2000-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a data frame for 200-2010 and creating a scatter plot\n",
    "years_to_2010 = by_years_df.loc[(by_years_df[\"Year Grouping\"] == \"2000-2010\"), :]\n",
    "\n",
    "#creating regression values\n",
    "(slope_2010, intercept_2010, rvalue_2010, pvalue_2010, stderr_2010) = st.linregress(\n",
    "    years_to_2010[\"Year\"], years_to_2010[\"Duration (seconds)\"])\n",
    "\n",
    "regress_value = years_to_2010[\"Year\"] * slope_2010 + intercept_2010\n",
    "\n",
    "#creating the linear regression equation\n",
    "line_eqn = \"y = \" + str(round(slope_2010,2)) + \"x + \" + str(round(intercept_2010,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3323f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a scatter plot with the regression line\n",
    "plt.scatter(years_to_2010[\"Year\"], years_to_2010[\"Duration (seconds)\"],)\n",
    "\n",
    "plt.plot(years_to_2010[\"Year\"], regress_value, \"r-\")\n",
    "plt.annotate(line_eqn, (2000,125), fontsize= 15,  color = \"red\")\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Length of Ad (seconds)\")\n",
    "plt.title(\"Length of SuperBowl Ads From 2000 to 2010\")\n",
    "plt.ylim(20,225)\n",
    "\n",
    "plt.xticks(np.arange(2000, 2011, step=1), rotation = 90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db324563",
   "metadata": {},
   "source": [
    "### 2011-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c08f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a data frame for 200-2010 and creating a scatter plot\n",
    "years_from_2011 = by_years_df.loc[(by_years_df[\"Year Grouping\"] == \"2011-2020\"), :]\n",
    "\n",
    "#creating regression values\n",
    "(slope_2011, intercept_2011, rvalue_2011, pvalue_2011, stderr_2011) = st.linregress(\n",
    "    years_from_2011[\"Year\"], years_from_2011[\"Duration (seconds)\"])\n",
    "\n",
    "regress_value = years_from_2011[\"Year\"] * slope_2011 + intercept_2011\n",
    "\n",
    "#creating the linear regression equation\n",
    "line_eqn = \"y = \" + str(round(slope_2011,2)) + \"x + \" + str(round(intercept_2011,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f46dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a scatter plot with the regression line\n",
    "plt.scatter(years_from_2011[\"Year\"], years_from_2011[\"Duration (seconds)\"],)\n",
    "\n",
    "plt.plot(years_from_2011[\"Year\"], regress_value, \"r-\")\n",
    "plt.annotate(line_eqn, (2011,150), fontsize= 15,  color = \"red\")\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Length of Ad (seconds)\")\n",
    "plt.title(\"Length of SuperBowl Ads From 2011 to 2020\")\n",
    "plt.ylim(20,225)\n",
    "\n",
    "plt.xticks(np.arange(2011, 2021, step=1), rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96d2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
